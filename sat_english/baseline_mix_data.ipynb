{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"baseline_mix_data.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.8"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WEiWqpvJzG22","executionInfo":{"status":"ok","timestamp":1606445840803,"user_tz":480,"elapsed":5804,"user":{"displayName":"J JS","photoUrl":"","userId":"03523663544813654809"}},"outputId":"89d902a0-1df4-4419-e4d2-bca4fc3e5b46"},"source":["import dill\n","from copy import deepcopy\n","import time\n","import random\n","import numpy as np\n","from sklearn.metrics import roc_curve, auc\n","\n","import nltk\n","\n","nltk.download(\"punkt\")\n","from nltk.tokenize import word_tokenize\n","\n","import torch\n","import torch.nn as nn\n","\n","from torchtext.data import Field\n","from torchtext.data import TabularDataset\n","from torchtext.data import BucketIterator\n","from torchtext.data import Iterator"],"execution_count":1,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DXvvT31XzRa4","executionInfo":{"status":"ok","timestamp":1606445840804,"user_tz":480,"elapsed":5800,"user":{"displayName":"J JS","photoUrl":"","userId":"03523663544813654809"}}},"source":["RANDOM_SEED = 2020\n","torch.manual_seed(RANDOM_SEED)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","np.random.seed(RANDOM_SEED)\n","random.seed(RANDOM_SEED)\n","\n","# DATA_PATH = \"data/processed/\"\n","DATA_PATH = \"/content/\""],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"fTocBve5qHZX","executionInfo":{"status":"ok","timestamp":1606445840805,"user_tz":480,"elapsed":5799,"user":{"displayName":"J JS","photoUrl":"","userId":"03523663544813654809"}}},"source":["import pandas as pd\n","\n","\n","train_df = []\n","valid_df = []\n","test_df = []\n","for data_name in [\"sat\", \"cola\"]:\n","    train_df += [pd.read_csv(f\"{DATA_PATH}/{data_name}_train.tsv\", sep=\"\\t\")]\n","    valid_df += [pd.read_csv(f\"{DATA_PATH}/{data_name}_valid.tsv\", sep=\"\\t\")]\n","\n","train_df = pd.concat(train_df)\n","valid_df = pd.concat(valid_df)\n","\n","train_df.to_csv(\"mix_train.tsv\", sep=\"\\t\", index=False)\n","valid_df.to_csv(\"mix_valid.tsv\", sep=\"\\t\", index=False)"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EvjX2S_WkRF8"},"source":["## Dataset"]},{"cell_type":"code","metadata":{"id":"k8Twwhw9zPdJ","executionInfo":{"status":"ok","timestamp":1606445842050,"user_tz":480,"elapsed":7042,"user":{"displayName":"J JS","photoUrl":"","userId":"03523663544813654809"}}},"source":["TEXT = Field(\n","    sequential=True,\n","    use_vocab=True,\n","    tokenize=word_tokenize,\n","    lower=True,\n","    batch_first=True,\n",")\n","LABEL = Field(\n","    sequential=False,\n","    use_vocab=False,\n","    batch_first=True,\n",")\n","\n","\n","mix_train_data, mix_valid_data = TabularDataset.splits(\n","    path=DATA_PATH,\n","    train=\"mix_train.tsv\",\n","    validation=\"mix_valid.tsv\",\n","    format=\"tsv\",\n","    fields=[(\"text\", TEXT), (\"label\", LABEL)],\n","    skip_header=1,\n",")\n","\n","TEXT.build_vocab(mix_train_data, min_freq=2)\n","\n","\n","mix_train_iterator, mix_valid_iterator = BucketIterator.splits(\n","    (mix_train_data, mix_valid_data),\n","    batch_size=32,\n","    device=None,\n","    sort=False,\n",")\n","\n","sat_test_data = TabularDataset(\n","    path=f\"{DATA_PATH}/sat_test.tsv\",\n","    format=\"tsv\",\n","    fields=[(\"text\", TEXT), (\"label\", LABEL)],\n","    skip_header=1\n",")\n","\n","sat_test_iterator = BucketIterator(\n","    sat_test_data,\n","    batch_size=8, \n","    device=None,\n","    sort=False,\n","    shuffle=False\n",")"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NaBRroeFkRF8"},"source":["## LSTM Classifier"]},{"cell_type":"code","metadata":{"id":"-ycMzmLHzbI9","executionInfo":{"status":"ok","timestamp":1606445842051,"user_tz":480,"elapsed":7041,"user":{"displayName":"J JS","photoUrl":"","userId":"03523663544813654809"}}},"source":["class LSTMClassifier(nn.Module):\n","    def __init__(self, num_embeddings, embedding_dim, hidden_size, num_layers, pad_idx):\n","        super().__init__()\n","        self.embed_layer = nn.Embedding(num_embeddings=num_embeddings, embedding_dim=embedding_dim, padding_idx=pad_idx)\n","        self.lstm_layer = nn.LSTM(\n","            input_size=embedding_dim, hidden_size=hidden_size, num_layers=num_layers, bidirectional=True, dropout=0.5\n","        )\n","        self.last_layer = nn.Sequential(\n","            nn.Linear(hidden_size * 2, hidden_size),\n","            nn.Dropout(0.5),\n","            nn.LeakyReLU(),\n","            nn.Linear(hidden_size, 1),\n","            nn.Sigmoid(),\n","        )\n","\n","    def forward(self, x):\n","        embed_x = self.embed_layer(x)\n","        output, (_, _) = self.lstm_layer(embed_x)\n","        last_output = output[:, -1, :]\n","        last_output = self.last_layer(last_output)\n","        return last_output"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"xHX9pXijzdKM","executionInfo":{"status":"ok","timestamp":1606445842051,"user_tz":480,"elapsed":7039,"user":{"displayName":"J JS","photoUrl":"","userId":"03523663544813654809"}}},"source":["def train(model: nn.Module, iterator: Iterator, optimizer: torch.optim.Optimizer, criterion: nn.Module, device: str):\n","    model.train()\n","\n","    epoch_loss = 0\n","\n","    for _, batch in enumerate(iterator):\n","        optimizer.zero_grad()\n","\n","        text = batch.text\n","        if text.shape[0] > 1:\n","            label = batch.label.type(torch.FloatTensor)\n","            text = text.to(device)\n","            label = label.to(device)\n","\n","            output = model(text).flatten()\n","            loss = criterion(output, label)\n","            loss.backward()\n","\n","            optimizer.step()\n","\n","            epoch_loss += loss.item()\n","\n","    return epoch_loss / len(iterator)\n","\n","\n","def evaluate(model: nn.Module, iterator: Iterator, criterion: nn.Module, device: str):\n","    model.eval()\n","    epoch_loss = 0\n","    with torch.no_grad():\n","        for _, batch in enumerate(iterator):\n","            text = batch.text\n","            label = batch.label.type(torch.FloatTensor)\n","            text = text.to(device)\n","            label = label.to(device)\n","            output = model(text).flatten()\n","            loss = criterion(output, label)\n","\n","            epoch_loss += loss.item()\n","\n","    return epoch_loss / len(iterator)\n","\n","\n","def test(model: nn.Module, iterator: Iterator, device: str):\n","\n","    with torch.no_grad():\n","        y_real = []\n","        y_pred = []\n","        model.eval()\n","        for batch in iterator:\n","            text = batch.text\n","            label = batch.label.type(torch.FloatTensor)\n","            text = text.to(device)\n","\n","            output = model(text).flatten().cpu()\n","\n","            y_real += [label]\n","            y_pred += [output]\n","\n","        y_real = torch.cat(y_real)\n","        y_pred = torch.cat(y_pred)\n","\n","    fpr, tpr, _ = roc_curve(y_real, y_pred)\n","    auroc = auc(fpr, tpr)\n","\n","    return auroc\n","\n","\n","def epoch_time(start_time: int, end_time: int):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nqYq9AypkRF9"},"source":["## Pretrain with cola dataset"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"so3L6wyQzeEI","executionInfo":{"status":"ok","timestamp":1606445982965,"user_tz":480,"elapsed":147923,"user":{"displayName":"J JS","photoUrl":"","userId":"03523663544813654809"}},"outputId":"f28b293a-4ff8-4f21-fcea-17bd77fe506a"},"source":["PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n","N_EPOCHS = 20\n","\n","lstm_classifier = LSTMClassifier(\n","    num_embeddings=len(TEXT.vocab),\n","    embedding_dim=100,\n","    hidden_size=200,\n","    num_layers=4,\n","    pad_idx=PAD_IDX,\n",")\n","if torch.cuda.is_available():\n","    device = \"cuda:0\"\n","else:\n","    device = \"cpu\"\n","_ = lstm_classifier.to(device)\n","\n","optimizer = torch.optim.Adam(lstm_classifier.parameters())\n","bce_loss_fn = nn.BCELoss()\n","\n","for epoch in range(N_EPOCHS):\n","\n","    start_time = time.time()\n","\n","    train_loss = train(lstm_classifier, mix_train_iterator, optimizer, bce_loss_fn, device)\n","    valid_loss = evaluate(lstm_classifier, mix_valid_iterator, bce_loss_fn, device)\n","\n","    end_time = time.time()\n","\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","\n","    print(f\"Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s\")\n","    print(f\"\\tTrain Loss: {train_loss:.5f}\")\n","    print(f\"\\t Val. Loss: {valid_loss:.5f}\")\n","\n","test_auroc = test(lstm_classifier, sat_test_iterator, device)\n","\n","print(f\"SAT Dataset Test AUROC: {test_auroc:.5f}\")"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Epoch: 01 | Time: 0m 6s\n","\tTrain Loss: 0.61367\n","\t Val. Loss: 0.63302\n","Epoch: 02 | Time: 0m 6s\n","\tTrain Loss: 0.61004\n","\t Val. Loss: 0.63028\n","Epoch: 03 | Time: 0m 6s\n","\tTrain Loss: 0.60872\n","\t Val. Loss: 0.63813\n","Epoch: 04 | Time: 0m 6s\n","\tTrain Loss: 0.64056\n","\t Val. Loss: 0.64096\n","Epoch: 05 | Time: 0m 6s\n","\tTrain Loss: 0.60925\n","\t Val. Loss: 0.64356\n","Epoch: 06 | Time: 0m 6s\n","\tTrain Loss: 0.60893\n","\t Val. Loss: 0.64212\n","Epoch: 07 | Time: 0m 6s\n","\tTrain Loss: 0.60907\n","\t Val. Loss: 0.64066\n","Epoch: 08 | Time: 0m 6s\n","\tTrain Loss: 0.60790\n","\t Val. Loss: 0.64121\n","Epoch: 09 | Time: 0m 6s\n","\tTrain Loss: 0.60805\n","\t Val. Loss: 0.64171\n","Epoch: 10 | Time: 0m 6s\n","\tTrain Loss: 0.60707\n","\t Val. Loss: 0.64598\n","Epoch: 11 | Time: 0m 6s\n","\tTrain Loss: 0.60602\n","\t Val. Loss: 0.65277\n","Epoch: 12 | Time: 0m 6s\n","\tTrain Loss: 0.60745\n","\t Val. Loss: 0.64338\n","Epoch: 13 | Time: 0m 6s\n","\tTrain Loss: 0.60841\n","\t Val. Loss: 0.64310\n","Epoch: 14 | Time: 0m 6s\n","\tTrain Loss: 0.60769\n","\t Val. Loss: 0.64272\n","Epoch: 15 | Time: 0m 6s\n","\tTrain Loss: 0.60878\n","\t Val. Loss: 0.64138\n","Epoch: 16 | Time: 0m 6s\n","\tTrain Loss: 0.60796\n","\t Val. Loss: 0.64071\n","Epoch: 17 | Time: 0m 6s\n","\tTrain Loss: 0.60795\n","\t Val. Loss: 0.64081\n","Epoch: 18 | Time: 0m 6s\n","\tTrain Loss: 0.60731\n","\t Val. Loss: 0.63998\n","Epoch: 19 | Time: 0m 6s\n","\tTrain Loss: 0.60752\n","\t Val. Loss: 0.64823\n","Epoch: 20 | Time: 0m 6s\n","\tTrain Loss: 0.60673\n","\t Val. Loss: 0.64014\n","| SAT Dataset Test AUROC: 0.50000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y_-a20XpdJZx","executionInfo":{"status":"ok","timestamp":1606445983183,"user_tz":480,"elapsed":148135,"user":{"displayName":"J JS","photoUrl":"","userId":"03523663544813654809"}},"outputId":"a5d002bd-0045-44da-c3cd-d715a2450e9d"},"source":["_ = lstm_classifier.cpu()\n","mix_lstm_sat_test_auroc = test(lstm_classifier, sat_test_iterator, \"cpu\")\n","\n","print(f\"SAT Dataset Test AUROC: {mix_lstm_sat_test_auroc:.5f}\")"],"execution_count":8,"outputs":[{"output_type":"stream","text":["| SAT Dataset Test AUROC: 0.50000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"v_LrY612srgT","executionInfo":{"status":"ok","timestamp":1606445983184,"user_tz":480,"elapsed":148133,"user":{"displayName":"J JS","photoUrl":"","userId":"03523663544813654809"}}},"source":["with open(\"mix_data_model.dill\", \"wb\") as f:\n","    model = {\n","        \"TEXT\": TEXT,\n","        \"LABEL\": LABEL,\n","        \"classifier\": lstm_classifier\n","    }\n","    dill.dump(model, f)"],"execution_count":9,"outputs":[]}]}